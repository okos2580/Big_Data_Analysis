"""
ì£¼ì‹ ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ
ìˆ˜ì§‘ëœ ì›ì‹œ ë°ì´í„°ë¥¼ ë¶„ì„ì— ì í•©í•œ í˜•íƒœë¡œ ì •ì œí•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
from datetime import datetime
import os

class StockDataPreprocessor:
    def __init__(self):
        """ë°ì´í„° ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”"""
        os.makedirs('data/processed', exist_ok=True)
        
    def load_data(self, filepath):
        """
        CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            filepath (str): CSV íŒŒì¼ ê²½ë¡œ
            
        Returns:
            pd.DataFrame: ë¡œë“œëœ ë°ì´í„°
        """
        try:
            data = pd.read_csv(filepath, encoding='utf-8-sig', index_col=0)
            # ì¸ë±ìŠ¤ë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
            data.index = pd.to_datetime(data.index)
            print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {filepath}")
            print(f"  - í–‰ ìˆ˜: {len(data)}")
            print(f"  - ì—´ ìˆ˜: {len(data.columns)}")
            return data
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def clean_data(self, data):
        """
        ê¸°ë³¸ì ì¸ ë°ì´í„° ì •ì œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            data (pd.DataFrame): ì›ì‹œ ë°ì´í„°
            
        Returns:
            pd.DataFrame: ì •ì œëœ ë°ì´í„°
        """
        cleaned_data = data.copy()
        
        # ê²°ì¸¡ì¹˜ í™•ì¸
        missing_values = cleaned_data.isnull().sum()
        if missing_values.sum() > 0:
            print("ê²°ì¸¡ì¹˜ ë°œê²¬:")
            print(missing_values[missing_values > 0])
            
            # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ë¥¼ ì´ì „ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            numeric_columns = cleaned_data.select_dtypes(include=[np.number]).columns
            cleaned_data[numeric_columns] = cleaned_data[numeric_columns].fillna(method='ffill')
            
            # ì—¬ì „íˆ ê²°ì¸¡ì¹˜ê°€ ìˆë‹¤ë©´ í‰ê· ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            cleaned_data[numeric_columns] = cleaned_data[numeric_columns].fillna(cleaned_data[numeric_columns].mean())
            
        # ì¤‘ë³µ í–‰ ì œê±°
        duplicate_count = cleaned_data.duplicated().sum()
        if duplicate_count > 0:
            print(f"ì¤‘ë³µ í–‰ {duplicate_count}ê°œ ì œê±°")
            cleaned_data = cleaned_data.drop_duplicates()
            
        # ìŒìˆ˜ ê±°ë˜ëŸ‰ ì œê±° (ìˆë‹¤ë©´)
        if 'Volume' in cleaned_data.columns:
            negative_volume = (cleaned_data['Volume'] < 0).sum()
            if negative_volume > 0:
                print(f"ìŒìˆ˜ ê±°ë˜ëŸ‰ {negative_volume}ê°œ ì œê±°")
                cleaned_data = cleaned_data[cleaned_data['Volume'] >= 0]
        
        print("âœ“ ê¸°ë³¸ ë°ì´í„° ì •ì œ ì™„ë£Œ")
        return cleaned_data
    
    def detect_outliers(self, data, column, method='iqr'):
        """
        ì´ìƒì¹˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
        
        Args:
            data (pd.DataFrame): ë°ì´í„°
            column (str): ì´ìƒì¹˜ë¥¼ ì°¾ì„ ì»¬ëŸ¼
            method (str): íƒì§€ ë°©ë²• ('iqr' ë˜ëŠ” 'zscore')
            
        Returns:
            pd.Series: ì´ìƒì¹˜ ì—¬ë¶€ (True/False)
        """
        if method == 'iqr':
            Q1 = data[column].quantile(0.25)
            Q3 = data[column].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = (data[column] < lower_bound) | (data[column] > upper_bound)
        
        elif method == 'zscore':
            z_scores = np.abs((data[column] - data[column].mean()) / data[column].std())
            outliers = z_scores > 3
            
        return outliers
    
    def calculate_technical_indicators(self, data):
        """
        ê¸°ìˆ ì  ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            data (pd.DataFrame): ì£¼ì‹ ë°ì´í„°
            
        Returns:
            pd.DataFrame: ê¸°ìˆ ì  ì§€í‘œê°€ ì¶”ê°€ëœ ë°ì´í„°
        """
        df = data.copy()
        
        # 1. ì´ë™í‰ê· ì„  (Moving Averages)
        df['MA_5'] = df['Close'].rolling(window=5).mean()
        df['MA_10'] = df['Close'].rolling(window=10).mean()
        df['MA_20'] = df['Close'].rolling(window=20).mean()
        df['MA_50'] = df['Close'].rolling(window=50).mean()
        
        # 2. ì§€ìˆ˜ ì´ë™í‰ê· ì„  (EMA)
        df['EMA_12'] = df['Close'].ewm(span=12).mean()
        df['EMA_26'] = df['Close'].ewm(span=26).mean()
        
        # 3. MACD
        df['MACD'] = df['EMA_12'] - df['EMA_26']
        df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()
        df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']
        
        # 4. RSI (Relative Strength Index)
        delta = df['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['RSI'] = 100 - (100 / (1 + rs))
        
        # 5. ë³¼ë¦°ì € ë°´ë“œ (Bollinger Bands)
        df['BB_Middle'] = df['Close'].rolling(window=20).mean()
        bb_std = df['Close'].rolling(window=20).std()
        df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)
        df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)
        df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']
        df['BB_Position'] = (df['Close'] - df['BB_Lower']) / df['BB_Width']
        
        # 6. ì¼ì¼ ìˆ˜ìµë¥ 
        df['Daily_Return'] = df['Close'].pct_change()
        
        # 7. ë³€ë™ì„± (20ì¼ ì´ë™ í‘œì¤€í¸ì°¨)
        df['Volatility'] = df['Daily_Return'].rolling(window=20).std() * np.sqrt(252)
        
        # 8. ê±°ë˜ëŸ‰ ì´ë™í‰ê· 
        df['Volume_MA'] = df['Volume'].rolling(window=20).mean()
        
        # 9. ê°€ê²© ë³€í™”ëŸ‰
        df['Price_Change'] = df['Close'] - df['Open']
        df['Price_Change_Pct'] = (df['Close'] - df['Open']) / df['Open'] * 100
        
        # 10. ê³ ê°€-ì €ê°€ ë²”ìœ„
        df['High_Low_Range'] = df['High'] - df['Low']
        df['High_Low_Range_Pct'] = (df['High'] - df['Low']) / df['Low'] * 100
        
        print("âœ“ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ")
        return df
    
    def add_time_features(self, data):
        """
        ì‹œê°„ ê´€ë ¨ íŠ¹ì„±ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Args:
            data (pd.DataFrame): ë°ì´í„°
            
        Returns:
            pd.DataFrame: ì‹œê°„ íŠ¹ì„±ì´ ì¶”ê°€ëœ ë°ì´í„°
        """
        df = data.copy()
        
        # ë‚ ì§œ ì¸ë±ìŠ¤ì—ì„œ ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ
        df['Year'] = df.index.year
        df['Month'] = df.index.month
        df['Day'] = df.index.day
        df['DayOfWeek'] = df.index.dayofweek  # 0: ì›”ìš”ì¼, 6: ì¼ìš”ì¼
        df['Quarter'] = df.index.quarter
        
        # ìš”ì¼ëª… ì¶”ê°€
        df['DayName'] = df.index.day_name()
        
        # ì›” ì´ˆ/ì¤‘/ë§ êµ¬ë¶„
        df['MonthPeriod'] = df['Day'].apply(lambda x: 'Early' if x <= 10 else 'Mid' if x <= 20 else 'Late')
        
        print("âœ“ ì‹œê°„ íŠ¹ì„± ì¶”ê°€ ì™„ë£Œ")
        return df
    
    def create_lag_features(self, data, columns, lags=[1, 2, 3, 5]):
        """
        ì§€ì—°(lag) íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            data (pd.DataFrame): ë°ì´í„°
            columns (list): ì§€ì—° íŠ¹ì„±ì„ ë§Œë“¤ ì»¬ëŸ¼ë“¤
            lags (list): ì§€ì—° ê¸°ê°„ë“¤
            
        Returns:
            pd.DataFrame: ì§€ì—° íŠ¹ì„±ì´ ì¶”ê°€ëœ ë°ì´í„°
        """
        df = data.copy()
        
        for col in columns:
            if col in df.columns:
                for lag in lags:
                    df[f'{col}_lag_{lag}'] = df[col].shift(lag)
        
        print(f"âœ“ ì§€ì—° íŠ¹ì„± ìƒì„± ì™„ë£Œ ({len(columns)}ê°œ ì»¬ëŸ¼ Ã— {len(lags)}ê°œ ì§€ì—°)")
        return df
    
    def normalize_data(self, data, method='minmax'):
        """
        ë°ì´í„°ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤.
        
        Args:
            data (pd.DataFrame): ë°ì´í„°
            method (str): ì •ê·œí™” ë°©ë²• ('minmax' ë˜ëŠ” 'zscore')
            
        Returns:
            pd.DataFrame: ì •ê·œí™”ëœ ë°ì´í„°
        """
        df = data.copy()
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        if method == 'minmax':
            # Min-Max ì •ê·œí™” (0-1 ë²”ìœ„)
            df[numeric_columns] = (df[numeric_columns] - df[numeric_columns].min()) / (df[numeric_columns].max() - df[numeric_columns].min())
        
        elif method == 'zscore':
            # Z-score ì •ê·œí™” (í‰ê·  0, í‘œì¤€í¸ì°¨ 1)
            df[numeric_columns] = (df[numeric_columns] - df[numeric_columns].mean()) / df[numeric_columns].std()
        
        print(f"âœ“ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ ({method} ë°©ë²•)")
        return df
    
    def preprocess_full_pipeline(self, data, include_lags=True, normalize=False):
        """
        ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            data (pd.DataFrame): ì›ì‹œ ë°ì´í„°
            include_lags (bool): ì§€ì—° íŠ¹ì„± í¬í•¨ ì—¬ë¶€
            normalize (bool): ì •ê·œí™” ìˆ˜í–‰ ì—¬ë¶€
            
        Returns:
            pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°
        """
        print("ğŸ”§ ì „ì²´ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        print("="*50)
        
        # 1. ê¸°ë³¸ ì •ì œ
        processed_data = self.clean_data(data)
        
        # 2. ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        processed_data = self.calculate_technical_indicators(processed_data)
        
        # 3. ì‹œê°„ íŠ¹ì„± ì¶”ê°€
        processed_data = self.add_time_features(processed_data)
        
        # 4. ì§€ì—° íŠ¹ì„± ìƒì„± (ì„ íƒì )
        if include_lags:
            lag_columns = ['Close', 'Volume', 'Daily_Return', 'RSI']
            processed_data = self.create_lag_features(processed_data, lag_columns)
        
        # 5. ì •ê·œí™” (ì„ íƒì )
        if normalize:
            processed_data = self.normalize_data(processed_data, method='minmax')
        
        # ê²°ì¸¡ì¹˜ ì œê±° (ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°ìœ¼ë¡œ ì¸í•œ ì´ˆê¸° NaN ê°’ë“¤)
        processed_data = processed_data.dropna()
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_data)}í–‰, {len(processed_data.columns)}ì—´")
        return processed_data
    
    def save_processed_data(self, data, filename):
        """
        ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            data (pd.DataFrame): ì „ì²˜ë¦¬ëœ ë°ì´í„°
            filename (str): ì €ì¥í•  íŒŒì¼ëª…
        """
        filepath = f"data/processed/{filename}"
        data.to_csv(filepath, encoding='utf-8-sig')
        print(f"ğŸ’¾ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥: {filepath}")
        return filepath

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    preprocessor = StockDataPreprocessor()
    
    print("ğŸ”§ ì£¼ì‹ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
    print("="*50)
    
    # ìµœê·¼ ìƒì„±ëœ ë°ì´í„° íŒŒì¼ ì°¾ê¸°
    raw_files = [f for f in os.listdir('data/raw/') if f.startswith('stock_data_combined_')]
    if not raw_files:
        print("âŒ ì²˜ë¦¬í•  ì›ì‹œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € data_collection.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    latest_file = max(raw_files)
    filepath = f"data/raw/{latest_file}"
    
    # ë°ì´í„° ë¡œë“œ
    raw_data = preprocessor.load_data(filepath)
    if raw_data is None:
        return
    
    # ì „ì²´ ì „ì²˜ë¦¬ ì‹¤í–‰
    processed_data = preprocessor.preprocess_full_pipeline(
        raw_data, 
        include_lags=True, 
        normalize=False
    )
    
    # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    processed_filename = f"processed_stock_data_{timestamp}.csv"
    preprocessor.save_processed_data(processed_data, processed_filename)
    
    # ë°ì´í„° ìš”ì•½ ì •ë³´ ì¶œë ¥
    print("\nğŸ“Š ì „ì²˜ë¦¬ëœ ë°ì´í„° ìš”ì•½:")
    print(f"  - ê¸°ê°„: {processed_data.index.min()} ~ {processed_data.index.max()}")
    print(f"  - ì¢…ëª© ìˆ˜: {processed_data['Symbol'].nunique()}")
    print(f"  - ì´ ë°ì´í„° í¬ì¸íŠ¸: {len(processed_data)}")
    print(f"  - íŠ¹ì„± ìˆ˜: {len(processed_data.columns)}")
    
    print("\nâœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 